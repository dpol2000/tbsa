{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 8. Parts of Speech 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I explore the most frequent words in the songs of the Beatles according to their classes and semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import operator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the dataset we must provide functions that would tranform the data into a format that can be used easily. For every word in a song a POS tag is needed. For POS identification I use <a href=\"https://spacy.io/\">SpaCy</a> again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences(song):\n",
    "    '''\n",
    "        Clean lyrics data and split into sentences\n",
    "    ''' \n",
    "    song_string = song.replace(\" cos \", \" 'cos \").replace(\"Cos \", \"'Cos \")\n",
    "    song_string = song_string.replace('[', '').replace(']', '')\n",
    "    song_string = song_string.replace('&#13;', '')\n",
    "    song_string = song_string.replace('<p>', '<br/>')\n",
    "    song_string = song_string.replace('</p>', '')\n",
    "    return [s.strip() for s in song_string.split('<br/>') if s]\n",
    "\n",
    "def get_pos(song, tags=True):\n",
    "    '''\n",
    "        Get part of speech tags for a song lyrics\n",
    "    '''\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for sentence in get_sentences(song):\n",
    "    \n",
    "        song_obj = nlp(sentence)\n",
    "        \n",
    "        if tags:\n",
    "            output.extend([token.tag_ for token in song_obj])\n",
    "        else:\n",
    "            output.extend([token.pos_ for token in song_obj])\n",
    "    \n",
    "    output = ' '.join(output) \n",
    "    return output\n",
    "\n",
    "def get_pos_words(song):\n",
    "    '''\n",
    "        Transform every word to its POS form, i.e., normalize it\n",
    "    '''\n",
    "    output = []\n",
    "    sentences = get_sentences(song)\n",
    "    for sentence in get_sentences(song):\n",
    "        song_obj = nlp(sentence)\n",
    "        output.extend([token.text for token in song_obj])\n",
    "    output = ' '.join(output) \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemmas(song):\n",
    "    '''\n",
    "        Get lemmatized words for a song\n",
    "    '''\n",
    "    output = []\n",
    "    sentences = get_sentences(song)\n",
    "    for sentence in get_sentences(song):\n",
    "        song_obj = nlp(sentence)\n",
    "        output.extend([token.lemma_ for token in song_obj])\n",
    "    output = ' '.join(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_post_words_list(cleaned_lyrics_pos_words, cleaned_lyrics_pos, pos_titles=[]):\n",
    "    '''\n",
    "        Transform data for a POS table\n",
    "    '''\n",
    "\n",
    "    pos_words = {}\n",
    "    \n",
    "    for song, song_pos in zip(cleaned_lyrics_pos_words, cleaned_lyrics_pos):\n",
    "        \n",
    "        song_list = song.split()\n",
    "        song_pos_list = song_pos.split() \n",
    "\n",
    "        for w, pos in zip(song_list, song_pos_list):\n",
    "\n",
    "            w = w.lower()\n",
    "            \n",
    "            if pos in pos_words:\n",
    "                if w in pos_words[pos]:\n",
    "                    pos_words[pos][w] = pos_words[pos][w] + 1\n",
    "                else:\n",
    "                    pos_words[pos][w] = 1\n",
    "            else:\n",
    "                pos_words[pos] = {}\n",
    "                pos_words[pos][w] = 1\n",
    "                \n",
    "    pos_words_results = {}\n",
    "    for pos, pos_word_dict in pos_words.items():\n",
    "        pos_sum = sum(pos_word_dict.values())\n",
    "        c = Counter(pos_word_dict)\n",
    "        raw_results = sorted(c.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        pos_words_results[pos] = [(result[0], round(100*float(result[1])/pos_sum, 2)) for result in raw_results]\n",
    "\n",
    "       \n",
    "    pos_words_list = []\n",
    "    pos_words_list_titles = []\n",
    "\n",
    "    for k,v in pos_words_results.items():\n",
    "        \n",
    "        if pos_titles:\n",
    "            \n",
    "            if k in pos_titles:\n",
    "                pos_words_list_titles.append(k)\n",
    "                pos_words_list.append(v)  \n",
    "        else:\n",
    "            pos_words_list_titles.append(k)\n",
    "            pos_words_list.append(v)  \n",
    "            \n",
    "        \n",
    "    return pos_words_list, pos_words_list_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_table(shares, limit, headlines, exclude=[]):\n",
    "    '''\n",
    "        Show an HTML table for data\n",
    "    '''\n",
    "    \n",
    "    words_by_column = '<tr>'\n",
    "    \n",
    "    excluded = []\n",
    "    \n",
    "    for i, headline in enumerate(headlines):\n",
    "        if headline in exclude:\n",
    "            excluded.append(i)\n",
    "        else:\n",
    "            words_by_column = words_by_column + '<td></td><td><strong>%s</strong></td>' % str(headline)\n",
    "    words_by_column = words_by_column + '</tr>'    \n",
    "    \n",
    "    if limit == 0:\n",
    "        limit = len(shares[0])\n",
    "    \n",
    "    for i in range(0, limit):\n",
    "        \n",
    "        words_by_column = words_by_column + '<tr>'\n",
    "\n",
    "        for j, st in enumerate(shares):\n",
    "\n",
    "            if j not in excluded:\n",
    "\n",
    "                if i < len(st):\n",
    "                    words_by_column = words_by_column + '<td>{}</td><td>{:.2f}</td>'.format(st[i][0], st[i][1])\n",
    "                else:\n",
    "                    words_by_column = words_by_column + '<td>-</td><td>-</td>'\n",
    "\n",
    "        words_by_column = words_by_column + '</tr>'    \n",
    "        \n",
    "    display(HTML('<table>' + words_by_column + '</table>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I did all the preparations needed to analyze the part of speech data. Now it's time to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cleaned_lyrics_pos'] = df['lyrics'].apply(get_pos, tags=False)\n",
    "df['cleaned_lyrics_pos_words'] = df['lyrics'].apply(get_pos_words)\n",
    "pos_words_list, pos_words_list_titles = get_post_words_list(df['cleaned_lyrics_pos_words'], \n",
    "                                                            df['cleaned_lyrics_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>ADV</strong></td><td></td><td><strong>DET</strong></td><td></td><td><strong>PART</strong></td><td></td><td><strong>NUM</strong></td><td></td><td><strong>PRON</strong></td><td></td><td><strong>NOUN</strong></td><td></td><td><strong>VERB</strong></td><td></td><td><strong>ADJ</strong></td><td></td><td><strong>INTJ</strong></td><td></td><td><strong>ADP</strong></td><td></td><td><strong>PROPN</strong></td><td></td><td><strong>CCONJ</strong></td></tr><tr><td>n't</td><td>17.17</td><td>the</td><td>41.90</td><td>to</td><td>49.04</td><td>one</td><td>33.05</td><td>you</td><td>30.43</td><td>love</td><td>4.94</td><td>do</td><td>5.50</td><td>my</td><td>13.30</td><td>oh</td><td>19.15</td><td>in</td><td>15.27</td><td>mm</td><td>3.39</td><td>and</td><td>75.62</td></tr><tr><td>so</td><td>6.17</td><td>a</td><td>28.54</td><td>on</td><td>8.72</td><td>two</td><td>15.25</td><td>i</td><td>30.30</td><td>what</td><td>3.32</td><td>'s</td><td>5.21</td><td>your</td><td>10.05</td><td>yeah</td><td>15.65</td><td>of</td><td>10.51</td><td>bill</td><td>2.58</td><td>but</td><td>18.98</td></tr><tr><td>now</td><td>5.70</td><td>all</td><td>8.10</td><td>up</td><td>8.16</td><td>four</td><td>6.78</td><td>me</td><td>12.85</td><td>baby</td><td>2.77</td><td>be</td><td>3.72</td><td>good</td><td>4.33</td><td>nah</td><td>11.49</td><td>to</td><td>8.62</td><td>c'mon</td><td>2.50</td><td>or</td><td>3.73</td></tr><tr><td>when</td><td>5.70</td><td>that</td><td>6.31</td><td>down</td><td>6.88</td><td>three</td><td>6.36</td><td>it</td><td>9.95</td><td>girl</td><td>2.66</td><td>know</td><td>3.62</td><td>little</td><td>3.42</td><td>well</td><td>7.66</td><td>for</td><td>7.67</td><td>bungalow</td><td>2.50</td><td>so</td><td>1.35</td></tr><tr><td>there</td><td>4.25</td><td>no</td><td>3.70</td><td>'s</td><td>6.64</td><td>five</td><td>5.08</td><td>she</td><td>5.51</td><td>time</td><td>2.42</td><td>'m</td><td>3.16</td><td>long</td><td>2.98</td><td>hey</td><td>5.68</td><td>with</td><td>7.45</td><td>da</td><td>2.23</td><td>yet</td><td>0.10</td></tr><tr><td>never</td><td>3.00</td><td>this</td><td>2.38</td><td>out</td><td>6.48</td><td>eight</td><td>4.66</td><td>we</td><td>2.79</td><td>day</td><td>1.69</td><td>is</td><td>3.03</td><td>that</td><td>2.77</td><td>ah</td><td>4.95</td><td>on</td><td>6.55</td><td>jude</td><td>2.05</td><td>n</td><td>0.10</td></tr><tr><td>back</td><td>2.97</td><td>some</td><td>1.56</td><td>na</td><td>6.24</td><td>seven</td><td>4.24</td><td>her</td><td>1.82</td><td>way</td><td>1.65</td><td>'ll</td><td>2.16</td><td>all</td><td>2.54</td><td>no</td><td>4.95</td><td>if</td><td>6.18</td><td>mr.</td><td>1.96</td><td>&</td><td>0.10</td></tr><tr><td>all</td><td>2.86</td><td>another</td><td>1.48</td><td>ta</td><td>3.44</td><td>six</td><td>4.24</td><td>they</td><td>1.81</td><td>man</td><td>1.55</td><td>got</td><td>2.08</td><td>her</td><td>2.47</td><td>yes</td><td>4.89</td><td>that</td><td>5.64</td><td>sgt</td><td>1.69</td><td>-</td><td>-</td></tr><tr><td>just</td><td>2.83</td><td>any</td><td>1.48</td><td>back</td><td>0.88</td><td>909</td><td>2.97</td><td>he</td><td>1.71</td><td>night</td><td>1.34</td><td>'re</td><td>1.89</td><td>much</td><td>2.37</td><td>please</td><td>4.49</td><td>like</td><td>3.16</td><td>la</td><td>1.60</td><td>-</td><td>-</td></tr><tr><td>too</td><td>2.78</td><td>an</td><td>1.36</td><td>by</td><td>0.72</td><td>ten</td><td>2.54</td><td>mine</td><td>0.69</td><td>things</td><td>0.99</td><td>can</td><td>1.77</td><td>his</td><td>2.17</td><td>ha</td><td>3.30</td><td>from</td><td>2.87</td><td>pepper</td><td>1.60</td><td>-</td><td>-</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_table(pos_words_list, 10, pos_words_list_titles, exclude=['PUNCT', 'X', 'SYM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can see the results (see full label descriptions <a href=\"https://spacy.io/api/annotation\">here</a>).\n",
    "\n",
    "**Nouns**: the leader is *love* (almost 5%). Then, there go *what*, *baby*, and *girl*, all roughly 3%. *Time* has about 2%, and about 1% is given to *way*, *man* and  *night*. All the other nouns have less that 1%.\n",
    "\n",
    "**Pronouns**: *you* is the leader (about 30%), *I* is very close. *Me* (~13%) and *it* (~10%) have significanlty lower results. Other pronouns have even lower figures.\n",
    "\n",
    "**Verbs**: here we see mostly forms of the verb *to be*: *do* is almost 6%, *'s* (which can be *is* or *has*, of course) is 5%, the *be* proper and *know* with about 4%, *'m* and *is* with about 3%, *'ll* and *got* with about 2%.\n",
    "\n",
    "**Adjectives**: the most popular are *my* (~13%) and *your*(~10%). These have been also described as possessive pronouns or possessive adjectives. Then, we see *good* (~4%), *little* (~3%) and *long* (~3%). Below we see pronouns again.\n",
    "\n",
    "**Adverbs**: the results are strange. Somehow *n't* has been classified as an adverb, and it has the first place with ~17%. Then we have *so* (~6%), *now* and *when* (both 5.7%). Other have less than 5%.\n",
    "\n",
    "**Adpositions** (both **prepositions** and **postpositions**): the leader is *in* (~15%), then we have *of* (~10.5%), then frequency slowly falls with *to*, *for*, *with*, *on*...\n",
    "\n",
    "**Particles**: *to* is the absolute leader with 49%! Then there are *on*, *up* and *down*.\n",
    "\n",
    "**Interjections**: *oh* (~19%) is more frequent than *yeah* (15.65%). *Nah* has the third place with 11.49% (because of *Hey Jude*, I guess).\n",
    "\n",
    "**Determintives**: *the* is much more frequent than a (42% vs. 29%).\n",
    "\n",
    "**Numbers**: *one* is the leader (almost 33%)! It's only natural that the second place (~15%) belongs to *two*. But *three* (6.75%) and *four* (6.33%) have reverse order. They are close anyway. Then funny enough we have *five* (5%), but then *eight* (4.64%). *Seven* and *six* have equal figures (4.22%).\n",
    "\n",
    "**Proper nouns**: there are clear signs of misclassification here. From the top ten only two (*Bill* and *Jude*) are attributed correctly.\n",
    "\n",
    "**Conjunctions**: *and* (76%) is the absolute leader! But the second place is given to *but* (~19%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look now at lemmatized lyrics frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cleaned_lyrics_lemmas'] = df['lyrics'].apply(get_lemmas)\n",
    "pos_lemmas_list, pos_lemmas_list_titles = get_post_words_list(df['cleaned_lyrics_lemmas'], df['cleaned_lyrics_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>NOUN</strong></td><td></td><td><strong>VERB</strong></td></tr><tr><td>love</td><td>5.03</td><td>be</td><td>20.49</td></tr><tr><td>what</td><td>3.32</td><td>do</td><td>6.82</td></tr><tr><td>girl</td><td>2.82</td><td>will</td><td>4.20</td></tr><tr><td>baby</td><td>2.77</td><td>know</td><td>4.05</td></tr><tr><td>time</td><td>2.56</td><td>get</td><td>3.71</td></tr><tr><td>day</td><td>2.02</td><td>have</td><td>3.34</td></tr><tr><td>way</td><td>1.72</td><td>go</td><td>3.25</td></tr><tr><td>man</td><td>1.58</td><td>can</td><td>2.85</td></tr><tr><td>night</td><td>1.38</td><td>say</td><td>2.55</td></tr><tr><td>thing</td><td>1.32</td><td>come</td><td>2.15</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_table(pos_lemmas_list, 10, pos_lemmas_list_titles, exclude=['PUNCT', 'DET', 'ADJ', 'CCONJ', 'ADP', 'PART',\n",
    "                                                                 'INTJ', 'ADV', 'X', 'NUM', 'SYM', 'PROPN', 'PRON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lemmatization has effect only on noun and verbs, so I examine only these.\n",
    "\n",
    "***Nouns***: lemmatization, as expected, hasn't changed much. The top ten is the same, but *girl* and *baby* have changed their places.\n",
    "\n",
    "***Verbs***: with all the forms of be summed up, *be* is he leader now with almost 21%. *Do* with its forms has less than 7%. *Will* is counted as a separate verb, and has 4%. Then we have *know*, *get*, *have*, *go* and others slowly falling down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's look at lemmatized nouns and verbs by authors and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>Lennon</strong></td><td></td><td><strong>McCartney</strong></td><td></td><td><strong>Harrison</strong></td></tr><tr><td>love</td><td>7.59</td><td>love</td><td>3.98</td><td>love</td><td>6.27</td></tr><tr><td>what</td><td>3.80</td><td>girl</td><td>3.06</td><td>sun</td><td>5.26</td></tr><tr><td>girl</td><td>3.21</td><td>time</td><td>2.68</td><td>time</td><td>5.01</td></tr><tr><td>nothing</td><td>2.55</td><td>what</td><td>2.29</td><td>what</td><td>5.01</td></tr><tr><td>morning</td><td>2.29</td><td>night</td><td>2.06</td><td>day</td><td>3.01</td></tr><tr><td>world</td><td>1.90</td><td>way</td><td>1.99</td><td>one</td><td>3.01</td></tr><tr><td>baby</td><td>1.77</td><td>day</td><td>1.99</td><td>because</td><td>3.01</td></tr><tr><td>everything</td><td>1.51</td><td>life</td><td>1.99</td><td>girl</td><td>2.76</td></tr><tr><td>mind</td><td>1.51</td><td>mother</td><td>1.83</td><td>thing</td><td>2.51</td></tr><tr><td>goo</td><td>1.44</td><td>road</td><td>1.76</td><td>taxman</td><td>2.51</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_pos_lemmas_list = []\n",
    "total_pos_lemmas_list_titles = []\n",
    "for writer in ('Lennon', 'McCartney', 'Harrison'):\n",
    "    pos_lemmas_list, pos_lemmas_list_titles = get_post_words_list(\n",
    "        df[df['writers']==writer]['cleaned_lyrics_lemmas'], \n",
    "        df[df['writers']==writer]['cleaned_lyrics_pos'], \n",
    "        pos_titles=['NOUN']\n",
    "    )\n",
    "    total_pos_lemmas_list.extend(pos_lemmas_list)\n",
    "    total_pos_lemmas_list_titles.append(writer)\n",
    "show_table(total_pos_lemmas_list, 10, total_pos_lemmas_list_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All you need is love, right? *Love* is the leader of course, but it seems that for McCartney the word *girl* (3.06) is almost as important as *love* (3.98%). The same goes for Harrison with distance from *sun* to *love* being 1%, while Lennon valued *love* much more. And only Lennon has the first top three identical to total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>Lennon</strong></td><td></td><td><strong>McCartney</strong></td><td></td><td><strong>Harrison</strong></td></tr><tr><td>be</td><td>23.05</td><td>be</td><td>18.73</td><td>be</td><td>22.79</td></tr><tr><td>do</td><td>6.43</td><td>do</td><td>7.70</td><td>do</td><td>8.55</td></tr><tr><td>know</td><td>4.31</td><td>will</td><td>5.54</td><td>know</td><td>4.68</td></tr><tr><td>go</td><td>4.02</td><td>go</td><td>4.20</td><td>will</td><td>4.48</td></tr><tr><td>get</td><td>3.54</td><td>know</td><td>3.83</td><td>have</td><td>4.17</td></tr><tr><td>can</td><td>3.41</td><td>get</td><td>3.46</td><td>come</td><td>3.15</td></tr><tr><td>say</td><td>2.89</td><td>say</td><td>3.37</td><td>go</td><td>2.95</td></tr><tr><td>come</td><td>2.80</td><td>have</td><td>3.21</td><td>get</td><td>2.34</td></tr><tr><td>have</td><td>2.67</td><td>let</td><td>2.50</td><td>want</td><td>2.24</td></tr><tr><td>will</td><td>2.15</td><td>see</td><td>2.33</td><td>see</td><td>1.93</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_pos_lemmas_list = []\n",
    "total_pos_lemmas_list_titles = []\n",
    "for writer in ('Lennon', 'McCartney', 'Harrison'):\n",
    "    pos_lemmas_list, pos_lemmas_list_titles = get_post_words_list(\n",
    "        df[df['writers']==writer]['cleaned_lyrics_lemmas'], \n",
    "        df[df['writers']==writer]['cleaned_lyrics_pos'], \n",
    "        pos_titles=['VERB']\n",
    "    )\n",
    "    total_pos_lemmas_list.extend(pos_lemmas_list)\n",
    "    total_pos_lemmas_list_titles.append(writer)\n",
    "show_table(total_pos_lemmas_list, 10, total_pos_lemmas_list_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All three share top two verbs: *be* and *do*, *be* being the absolute leader. But then differencies begin. Both Lennon and McCartney value *go* (the fourth place), while it has only 7th place for Harrison. Lennon has *can* at the 6th place, while the others don't have it in the top ten at all.\n",
    "\n",
    "Let's examine lemmatized nouns year by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>1963</strong></td><td></td><td><strong>1964</strong></td><td></td><td><strong>1965</strong></td><td></td><td><strong>1966</strong></td><td></td><td><strong>1967</strong></td><td></td><td><strong>1968</strong></td><td></td><td><strong>1969</strong></td><td></td><td><strong>1970</strong></td></tr><tr><td>love</td><td>7.05</td><td>love</td><td>8.94</td><td>girl</td><td>8.30</td><td>day</td><td>6.86</td><td>love</td><td>7.35</td><td>what</td><td>3.24</td><td>sun</td><td>4.39</td><td>everybody</td><td>5.38</td></tr><tr><td>baby</td><td>5.72</td><td>baby</td><td>4.70</td><td>what</td><td>5.77</td><td>submarine</td><td>5.82</td><td>morning</td><td>3.57</td><td>life</td><td>2.19</td><td>love</td><td>4.02</td><td>feeling</td><td>5.06</td></tr><tr><td>girl</td><td>4.26</td><td>time</td><td>4.36</td><td>love</td><td>3.81</td><td>love</td><td>4.16</td><td>name</td><td>2.83</td><td>girl</td><td>1.88</td><td>what</td><td>3.11</td><td>way</td><td>4.75</td></tr><tr><td>what</td><td>3.99</td><td>what</td><td>4.01</td><td>time</td><td>3.69</td><td>paperback</td><td>3.95</td><td>nothing</td><td>2.62</td><td>night</td><td>1.88</td><td>way</td><td>2.38</td><td>world</td><td>4.11</td></tr><tr><td>boy</td><td>3.46</td><td>thing</td><td>3.78</td><td>baby</td><td>3.69</td><td>writer</td><td>3.95</td><td>mother</td><td>2.52</td><td>birthday</td><td>1.67</td><td>bom</td><td>2.01</td><td>nothing</td><td>4.11</td></tr><tr><td>man</td><td>2.93</td><td>day</td><td>3.10</td><td>word</td><td>3.34</td><td>sunshine</td><td>3.74</td><td>man</td><td>2.52</td><td>dream</td><td>1.67</td><td>darling</td><td>2.01</td><td>girl</td><td>2.85</td></tr><tr><td>minute</td><td>2.53</td><td>honey</td><td>2.64</td><td>way</td><td>2.88</td><td>what</td><td>3.33</td><td>time</td><td>2.52</td><td>road</td><td>1.57</td><td>child</td><td>1.83</td><td>word</td><td>2.53</td></tr><tr><td>heart</td><td>2.53</td><td>girl</td><td>2.41</td><td>night</td><td>2.77</td><td>people</td><td>2.70</td><td>goo</td><td>2.31</td><td>gun</td><td>1.57</td><td>time</td><td>1.65</td><td>time</td><td>2.53</td></tr><tr><td>time</td><td>2.13</td><td>way</td><td>2.41</td><td>day</td><td>2.42</td><td>time</td><td>2.49</td><td>sky</td><td>2.10</td><td>mind</td><td>1.46</td><td>something</td><td>1.65</td><td>everything</td><td>2.22</td></tr><tr><td>shuop</td><td>1.99</td><td>because</td><td>2.41</td><td>mind</td><td>1.96</td><td>life</td><td>2.29</td><td>baby</td><td>1.99</td><td>baby</td><td>1.36</td><td>garden</td><td>1.65</td><td>wisdom</td><td>2.22</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_pos_lemmas_list = []\n",
    "total_pos_lemmas_list_titles = []\n",
    "for year in range(1963, 1971):\n",
    "    pos_lemmas_list, pos_lemmas_list_titles = get_post_words_list(\n",
    "        df[df['year']==year]['cleaned_lyrics_lemmas'], \n",
    "        df[df['year']==year]['cleaned_lyrics_pos'], \n",
    "        pos_titles=['NOUN']\n",
    "    )\n",
    "    total_pos_lemmas_list.extend(pos_lemmas_list)\n",
    "    total_pos_lemmas_list_titles.append(str(year))\n",
    "show_table(total_pos_lemmas_list, 10, total_pos_lemmas_list_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note that the absolute leader *love* wasn't always one. In fact, *love* is the leader only for three years, and in the other years there are many others - *girl*, *day*, *what*, *sun*, *everybody*. In 1968 and 1970 *love* isn't even in the top ten.\n",
    "\n",
    "Now it's time to look at verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>1963</strong></td><td></td><td><strong>1964</strong></td><td></td><td><strong>1965</strong></td><td></td><td><strong>1966</strong></td><td></td><td><strong>1967</strong></td><td></td><td><strong>1968</strong></td><td></td><td><strong>1969</strong></td><td></td><td><strong>1970</strong></td></tr><tr><td>be</td><td>18.80</td><td>be</td><td>19.36</td><td>be</td><td>22.37</td><td>be</td><td>20.54</td><td>be</td><td>23.98</td><td>be</td><td>17.24</td><td>be</td><td>21.46</td><td>be</td><td>21.34</td></tr><tr><td>will</td><td>7.79</td><td>do</td><td>7.49</td><td>do</td><td>5.43</td><td>do</td><td>6.04</td><td>know</td><td>7.00</td><td>do</td><td>11.41</td><td>do</td><td>8.99</td><td>get</td><td>9.28</td></tr><tr><td>do</td><td>5.51</td><td>will</td><td>5.24</td><td>will</td><td>5.02</td><td>know</td><td>3.89</td><td>do</td><td>5.50</td><td>know</td><td>5.58</td><td>come</td><td>5.39</td><td>let</td><td>7.17</td></tr><tr><td>get</td><td>4.69</td><td>have</td><td>4.65</td><td>have</td><td>4.82</td><td>will</td><td>3.76</td><td>need</td><td>4.28</td><td>come</td><td>4.29</td><td>go</td><td>4.55</td><td>have</td><td>6.35</td></tr><tr><td>love</td><td>4.22</td><td>get</td><td>4.06</td><td>go</td><td>3.96</td><td>can</td><td>3.49</td><td>get</td><td>4.00</td><td>go</td><td>3.85</td><td>know</td><td>4.44</td><td>go</td><td>4.07</td></tr><tr><td>know</td><td>4.16</td><td>love</td><td>3.69</td><td>can</td><td>3.65</td><td>have</td><td>3.09</td><td>say</td><td>3.57</td><td>make</td><td>3.01</td><td>get</td><td>3.70</td><td>say</td><td>3.75</td></tr><tr><td>want</td><td>2.75</td><td>can</td><td>3.64</td><td>see</td><td>3.25</td><td>say</td><td>2.68</td><td>go</td><td>3.43</td><td>will</td><td>2.63</td><td>want</td><td>3.28</td><td>can</td><td>2.61</td></tr><tr><td>can</td><td>2.52</td><td>go</td><td>2.99</td><td>get</td><td>2.84</td><td>get</td><td>2.55</td><td>can</td><td>2.43</td><td>have</td><td>2.63</td><td>can</td><td>2.54</td><td>dig</td><td>2.28</td></tr><tr><td>wanna</td><td>2.46</td><td>know</td><td>2.78</td><td>say</td><td>2.69</td><td>need</td><td>2.42</td><td>see</td><td>2.36</td><td>d'do</td><td>2.50</td><td>say</td><td>2.54</td><td>change</td><td>1.95</td></tr><tr><td>come</td><td>2.34</td><td>say</td><td>2.62</td><td>know</td><td>2.69</td><td>want</td><td>2.28</td><td>have</td><td>2.36</td><td>see</td><td>1.99</td><td>let</td><td>2.33</td><td>do</td><td>1.95</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_pos_lemmas_list = []\n",
    "total_pos_lemmas_list_titles = []\n",
    "for year in range(1963, 1971):\n",
    "    pos_lemmas_list, pos_lemmas_list_titles = get_post_words_list(\n",
    "        df[df['year']==year]['cleaned_lyrics_lemmas'], \n",
    "        df[df['year']==year]['cleaned_lyrics_pos'], \n",
    "        pos_titles=['VERB']\n",
    "    )\n",
    "    total_pos_lemmas_list.extend(pos_lemmas_list)\n",
    "    total_pos_lemmas_list_titles.append(str(year))\n",
    "show_table(total_pos_lemmas_list, 10, total_pos_lemmas_list_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *be* is the absolute leader for all years. *Do* occupies the second place, but sometimes loses it to *will* (1963), *know* (1967), or *get* (1970).\n",
    "\n",
    "Now let's look at the difference between covers and original songs. First, nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>Covers</strong></td><td></td><td><strong>Original</strong></td></tr><tr><td>baby</td><td>9.21</td><td>love</td><td>5.44</td></tr><tr><td>honey</td><td>3.84</td><td>what</td><td>3.28</td></tr><tr><td>what</td><td>3.58</td><td>girl</td><td>2.84</td></tr><tr><td>girl</td><td>2.69</td><td>time</td><td>2.66</td></tr><tr><td>way</td><td>2.69</td><td>day</td><td>2.24</td></tr><tr><td>love</td><td>2.43</td><td>baby</td><td>1.75</td></tr><tr><td>minute</td><td>2.43</td><td>man</td><td>1.71</td></tr><tr><td>rock</td><td>2.30</td><td>way</td><td>1.57</td></tr><tr><td>boy</td><td>2.17</td><td>thing</td><td>1.47</td></tr><tr><td>shuop</td><td>1.92</td><td>night</td><td>1.39</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cover = df[df.cover==True]\n",
    "df_orig = df[df.cover==False]\n",
    "\n",
    "total_pos_lemmas_list = []\n",
    "total_pos_lemmas_list_titles = []\n",
    "\n",
    "pos_lemmas_list1, pos_lemmas_list_titles = get_post_words_list(\n",
    "    df_cover['cleaned_lyrics_lemmas'], \n",
    "    df_cover['cleaned_lyrics_pos'], \n",
    "    pos_titles=['NOUN']\n",
    ")\n",
    "\n",
    "pos_lemmas_list2, pos_lemmas_list_titles = get_post_words_list(\n",
    "    df_orig['cleaned_lyrics_lemmas'], \n",
    "    df_orig['cleaned_lyrics_pos'], \n",
    "    pos_titles=['NOUN']\n",
    ")\n",
    "\n",
    "total_pos_lemmas_list_titles = ['Covers', 'Original']\n",
    "total_pos_lemmas_list.extend(pos_lemmas_list1)\n",
    "total_pos_lemmas_list.extend(pos_lemmas_list2)\n",
    "\n",
    "show_table(total_pos_lemmas_list, 10, total_pos_lemmas_list_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's funny that covers which are mostly love songs don't list *love* even in the top ten! The absolute leader is *baby*. Original songs have traditional leaders as *love* and *what*. But what about verbs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td></td><td><strong>Covers</strong></td><td></td><td><strong>Original</strong></td></tr><tr><td>be</td><td>17.55</td><td>be</td><td>20.86</td></tr><tr><td>get</td><td>6.54</td><td>do</td><td>6.88</td></tr><tr><td>do</td><td>6.37</td><td>know</td><td>4.30</td></tr><tr><td>will</td><td>4.14</td><td>will</td><td>4.21</td></tr><tr><td>want</td><td>3.15</td><td>have</td><td>3.38</td></tr><tr><td>go</td><td>3.06</td><td>get</td><td>3.35</td></tr><tr><td>have</td><td>2.98</td><td>go</td><td>3.27</td></tr><tr><td>say</td><td>2.57</td><td>can</td><td>2.89</td></tr><tr><td>come</td><td>2.48</td><td>say</td><td>2.55</td></tr><tr><td>can</td><td>2.48</td><td>come</td><td>2.10</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_pos_lemmas_list = []\n",
    "total_pos_lemmas_list_titles = []\n",
    "\n",
    "pos_lemmas_list1, pos_lemmas_list_titles = get_post_words_list(\n",
    "    df_cover['cleaned_lyrics_lemmas'], \n",
    "    df_cover['cleaned_lyrics_pos'], \n",
    "    pos_titles=['VERB']\n",
    ")\n",
    "\n",
    "pos_lemmas_list2, pos_lemmas_list_titles = get_post_words_list(\n",
    "    df_orig['cleaned_lyrics_lemmas'], \n",
    "    df_orig['cleaned_lyrics_pos'], \n",
    "    pos_titles=['VERB']\n",
    ")\n",
    "\n",
    "total_pos_lemmas_list_titles = ['Covers', 'Original']\n",
    "total_pos_lemmas_list.extend(pos_lemmas_list1)\n",
    "total_pos_lemmas_list.extend(pos_lemmas_list2)\n",
    "\n",
    "show_table(total_pos_lemmas_list, 10, total_pos_lemmas_list_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While *be* in an undisputable leader, covers have surprisingly high frequency for *get* (the second place!) and low (not even in top ten) for *know*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>CONCLUSION</h2>\n",
    "\n",
    "If one splits word frequencies according to parts of speech, one can see the following. For nouns, the leader is *love* (almost 5% though). For pronouns, *you* is the leader (about 30%), and *I* is very close. As for verbs, while the leader is *do* (almost 6%), in the top ten there are mostly forms of the verb *to be*.\n",
    "\n",
    "In determintives, *the* is much more frequent than *a* (42% vs. 29%). From the top ten of proper nouns only two (*Bill* and *Jude*) are classified correctly. In conjunctions *and* (76%) is the absolute leader, while the second place is given to *but* (~19%).\n",
    "\n",
    "The most popular adjectives are *my* (~13%) and *your* (~10%).\n",
    "\n",
    "In adverbs, the results are strange. As *n't* has been classified as an adverb, it has the first place with ~17%. Then we have *so* (~6%), *now* and *when* (both 5.7%).\n",
    "\n",
    "As for adpositions, the leader is *in* (~15%). Among particles, *to* is the absolute leader with 49%! And in interjections *oh* (~19%) is more frequent than *yeah* (15.65%). *Nah* has the third place with 11.49%.\n",
    "\n",
    "Lemmatization changes almost nothing for nouns. As for verbs, *be* is the leader now with almost 21%. *Do* with its forms has less than 7%.\n",
    "\n",
    "Looking at time, it's interesting to note that the absolute leader *love* wasn't always one. In fact, *love* is the leader only in three years, and in the other years there are many other leaders - *girl*, *day*, *what*, *sun*, *everybody*. As for verbs, *be* is the absolute leader for all years. *Do* holds the second place, but sometimes loses it to other words.\n",
    "\n",
    "It's interesting that covers' nouns don't even list *love* in the top ten! The absolute leader there is *baby*.\n",
    "\n",
    "*Love* is the leader in nouns for all the three original authors, and as for verbs, while *be* in an undisputable leader, covers have surprisingly high frequency for *get* (the second place!) and low (not even in top ten) for *know*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
